# GDPæ—¶é—´åºåˆ—é¢„æµ‹ç³»ç»ŸæŠ€æœ¯æ–‡æ¡£

## ç›®å½•

1. [é¡¹ç›®æ¦‚è¿°](#é¡¹ç›®æ¦‚è¿°)
2. [ç³»ç»Ÿæ¶æ„](#ç³»ç»Ÿæ¶æ„)
3. [æ¨¡å—è¯¦è§£](#æ¨¡å—è¯¦è§£)
4. [ä½¿ç”¨æŒ‡å—](#ä½¿ç”¨æŒ‡å—)
5. [APIå‚è€ƒ](#apiå‚è€ƒ)

---

## é¡¹ç›®æ¦‚è¿°

### é¡¹ç›®ç®€ä»‹

æœ¬é¡¹ç›®æ˜¯ä¸€ä¸ªåŸºäºæ·±åº¦å­¦ä¹ çš„GDPé¢„æµ‹ç³»ç»Ÿ,ä½¿ç”¨Seq2Seq(åºåˆ—åˆ°åºåˆ—)æ¶æ„çš„LSTMç¥ç»ç½‘ç»œ,é€šè¿‡åˆ†æå†å²ç»æµæ•°æ®é¢„æµ‹æœªæ¥GDPèµ°åŠ¿ã€‚

### æ ¸å¿ƒåŠŸèƒ½

- ğŸ“Š å¤šç»´åº¦ç»æµæŒ‡æ ‡æ•°æ®å¤„ç†(äººå£ã€æ¶ˆè´¹ã€GDPã€è´¢æ”¿)
- ğŸ¤– åŸºäºLSTMçš„Seq2Seqæ—¶é—´åºåˆ—é¢„æµ‹æ¨¡å‹
- ğŸ“ˆ è®­ç»ƒè¿‡ç¨‹å¯è§†åŒ–ä¸æŒ‡æ ‡ç›‘æ§
- ğŸ—ºï¸ æ”¯æŒå…¨å›½31ä¸ªçœçº§è¡Œæ”¿åŒºåŸŸ
- ğŸ’¾ æ¨¡å‹æŒä¹…åŒ–(PyTorchæ ¼å¼ä¸ONNXæ ¼å¼)
- ğŸ“‰ é¢„æµ‹ç»“æœå¯è§†åŒ–å¯¹æ¯”

### æŠ€æœ¯æ ˆ

| ç±»åˆ« | æŠ€æœ¯ |
|------|------|
| æ·±åº¦å­¦ä¹ æ¡†æ¶ | PyTorch 2.x |
| æ•°æ®å¤„ç† | Pandas, NumPy |
| æ•°æ®æ ‡å‡†åŒ– | Scikit-learn |
| å¯è§†åŒ– | Matplotlib |
| æ¨¡å‹å¯¼å‡º | ONNX |

---

## ç³»ç»Ÿæ¶æ„

### æ•´ä½“æµç¨‹å›¾

```
æ•°æ®è¯»å– â†’ æ•°æ®é¢„å¤„ç† â†’ åºåˆ—æ„å»º â†’ æ¨¡å‹è®­ç»ƒ â†’ é¢„æµ‹è¯„ä¼° â†’ ç»“æœä¿å­˜
   â†“           â†“            â†“           â†“          â†“          â†“
data_setup  MinMaxæ ‡å‡†åŒ–  utils.py   engine.py  å¯è§†åŒ–   JSON/ONNX
```

### ç›®å½•ç»“æ„

```
project/
â”œâ”€â”€ data_setup.py      # æ•°æ®åŠ è½½ä¸é¢„å¤„ç†
â”œâ”€â”€ model_builder.py   # Seq2Seqæ¨¡å‹å®šä¹‰
â”œâ”€â”€ engine.py          # è®­ç»ƒä¸é¢„æµ‹å¼•æ“
â”œâ”€â”€ utils.py           # å·¥å…·å‡½æ•°(åºåˆ—æ„å»ºã€å¯è§†åŒ–)
â”œâ”€â”€ train.py           # ä¸»è®­ç»ƒè„šæœ¬
â””â”€â”€ data/              # æ•°æ®ç›®å½•
    â”œâ”€â”€ YearPeople.csv     # äººå£æ•°æ®
    â”œâ”€â”€ YearXiaofei.csv    # æ¶ˆè´¹æ•°æ®
    â”œâ”€â”€ YearGDP.csv        # GDPæ•°æ®
    â””â”€â”€ YearFinancial.csv  # è´¢æ”¿æ•°æ®
```

---

## æ¨¡å—è¯¦è§£

## 1. data_setup.py - æ•°æ®åŠ è½½æ¨¡å—

### åŠŸèƒ½æ¦‚è¿°

è´Ÿè´£ä»CSVæ–‡ä»¶ä¸­è¯»å–å¤šç»´ç»æµæ•°æ®,è¿›è¡Œåˆå¹¶ã€å½’ä¸€åŒ–å¤„ç†ã€‚

### æ ¸å¿ƒå‡½æ•°

#### `create_dataset(data_path: str, province: str)`

**åŠŸèƒ½**: åˆ›å»ºæŒ‡å®šçœä»½çš„æ ‡å‡†åŒ–æ•°æ®é›†

**å‚æ•°**:
- `data_path` (str): æ•°æ®æ–‡ä»¶å¤¹è·¯å¾„
- `province` (str): çœä»½åç§°(å¦‚"åŒ—äº¬å¸‚")

**è¿”å›å€¼**:
- `origin_data_reversed` (DataFrame): åŸå§‹æ•°æ®(æ—¶é—´é€†åº)
- `data_scaled` (ndarray): å½’ä¸€åŒ–åçš„æ•°æ®

**å¤„ç†æµç¨‹**:

```python
1. è¯»å–4ä¸ªCSVæ–‡ä»¶(äººå£ã€æ¶ˆè´¹ã€GDPã€è´¢æ”¿)
2. æå–ç›®æ ‡çœä»½åˆ—æ•°æ®
3. åˆå¹¶ä¸ºDataFrame(4åˆ—ç‰¹å¾)
4. æ—¶é—´é€†åºæ’åˆ—(è¿œâ†’è¿‘)
5. MinMaxå½’ä¸€åŒ–å¤„ç†[0,1]
```

**æ•°æ®æ ¼å¼ç¤ºä¾‹**:

| polulation | consumption | GDP | financial |
|-----------|-------------|-----|-----------|
| 1633.0    | 8542.3      | 12406.8 | 2353.1 |
| 1695.0    | 9353.3      | 14113.6 | 2565.0 |

**å…³é”®ä»£ç è§£æ**:

```python
# æ—¶é—´åºåˆ—åè½¬(æœ€æ–°æ•°æ®åœ¨å‰)
origin_data_reversed = origin_data.iloc[::-1].reset_index(drop=True)

# MinMaxæ ‡å‡†åŒ–: X' = (X - X_min) / (X_max - X_min)
scaler = MinMaxScaler()
data_scaled = scaler.fit_transform(data_for_scaling)
```

---

## 2. model_builder.py - æ¨¡å‹å®šä¹‰æ¨¡å—

### Seq2Seqæ¶æ„åŸç†

#### ç¼–ç å™¨-è§£ç å™¨ç»“æ„

```
ç¼–ç å™¨(Encoder)           è§£ç å™¨(Decoder)
å†å²åºåˆ— â†’ LSTM â†’ éšçŠ¶æ€ â†’ LSTM â†’ æœªæ¥é¢„æµ‹
[t-5...t]               [t+1,t+2]
```

### æ ¸å¿ƒç±»: `Seq2Seq`

#### æ¨¡å‹å‚æ•°

| å‚æ•° | è¯´æ˜ | é»˜è®¤å€¼ |
|-----|------|--------|
| input_size | è¾“å…¥ç‰¹å¾æ•° | 4(äººå£/æ¶ˆè´¹/GDP/è´¢æ”¿) |
| hidden_size | éšè—å±‚ç¥ç»å…ƒæ•° | 8 |
| num_layers | LSTMå †å å±‚æ•° | 2 |
| output_size | è¾“å‡ºç‰¹å¾æ•° | 1(ä»…GDP) |
| predict_steps | é¢„æµ‹æ­¥æ•° | 2(é¢„æµ‹2å¹´) |

#### å‰å‘ä¼ æ’­æµç¨‹

```python
def forward(self, x):
    # xå½¢çŠ¶: (batch_size, window_size, 4)
    
    # é˜¶æ®µ1: ç¼–ç å†å²åºåˆ—
    _, (enc_hn, enc_cn) = self.encoder(x)
    # enc_hnå½¢çŠ¶: (num_layers, batch_size, hidden_size)
    
    # é˜¶æ®µ2: åˆå§‹åŒ–è§£ç å™¨
    predictions = torch.zeros(batch_size, predict_steps, 1)
    dec_input = torch.zeros(batch_size, 1, 1)  # åˆå§‹è¾“å…¥ä¸º0
    
    # é˜¶æ®µ3: è‡ªå›å½’é¢„æµ‹
    for t in range(predict_steps):
        dec_output, (hn, cn) = self.decoder(dec_input, (hn, cn))
        pred = self.fc(dec_output)
        predictions[:, t, :] = pred
        dec_input = pred  # ç”¨é¢„æµ‹å€¼ä½œä¸ºä¸‹ä¸€æ­¥è¾“å…¥
    
    return predictions  # (batch_size, 2, 1)
```

#### ç½‘ç»œç»“æ„å›¾

```
è¾“å…¥å±‚(4ç»´)
    â†“
LSTMç¼–ç å™¨(8â†’8)
    â†“
éšçŠ¶æ€ä¼ é€’
    â†“
LSTMè§£ç å™¨(1â†’8)
    â†“
å…¨è¿æ¥å±‚(8â†’1)
    â†“
è¾“å‡º(GDPé¢„æµ‹)
```

---

## 3. engine.py - è®­ç»ƒå¼•æ“æ¨¡å—

### 3.1 è¯„ä¼°æŒ‡æ ‡

#### æ€§èƒ½åº¦é‡å‡½æ•°

| æŒ‡æ ‡ | å…¬å¼ | è¯´æ˜ |
|-----|------|------|
| MAE | $\frac{1}{n}\sum \|y_i - \hat{y}_i \|$ | å¹³å‡ç»å¯¹è¯¯å·® |
| MSE | $\frac{1}{n}\sum(y_i - \hat{y}_i)^2$ | å‡æ–¹è¯¯å·® |
| MAPE | $\frac{100}{n}\sum\|\frac{y_i - \hat{y}_i}{y_i}\|$ | å¹³å‡ç»å¯¹ç™¾åˆ†æ¯”è¯¯å·® |

#### ä»£ç å®ç°

```python
def mae_metric(predictions, targets):
    return torch.nn.functional.l1_loss(predictions, targets)

def mse_metric(predictions, targets):
    return torch.nn.functional.mse_loss(predictions, targets)

def mape_metric(predictions, targets):
    epsilon = 1e-8  # é˜²æ­¢é™¤é›¶
    return torch.mean(torch.abs((targets - predictions) / (targets + epsilon))) * 10
```

### 3.2 è®­ç»ƒæµç¨‹

#### `train_step()` - å•è½®è®­ç»ƒ

```python
å…³é”®æ­¥éª¤:
1. è®¾ç½®æ¨¡å‹ä¸ºè®­ç»ƒæ¨¡å¼(model.train())
2. éå†æ¯ä¸ªè®­ç»ƒæ ·æœ¬:
   - è½¬æ¢ä¸ºTensorå¹¶æ·»åŠ æ‰¹æ¬¡ç»´åº¦
   - å‰å‘ä¼ æ’­è®¡ç®—é¢„æµ‹å€¼
   - è®¡ç®—æŸå¤±(MSE)å’Œè¯„ä¼°æŒ‡æ ‡
   - åå‘ä¼ æ’­æ›´æ–°å‚æ•°
3. è¿”å›å¹³å‡æŒ‡æ ‡
```

#### `train()` - å®Œæ•´è®­ç»ƒå¾ªç¯

```python
è®­ç»ƒç›‘æ§æŒ‡æ ‡:
- train_loss: è®­ç»ƒé›†MSEæŸå¤±
- train_mae: è®­ç»ƒé›†å¹³å‡ç»å¯¹è¯¯å·®
- train_mse: è®­ç»ƒé›†å‡æ–¹è¯¯å·®
- train_mape: è®­ç»ƒé›†å¹³å‡ç»å¯¹ç™¾åˆ†æ¯”è¯¯å·®
- test_*: å¯¹åº”çš„æµ‹è¯•é›†æŒ‡æ ‡
```

### 3.3 é¢„æµ‹å‡½æ•°

#### `predict()` - æ¨¡å‹æ¨ç†

**æ ¸å¿ƒé€»è¾‘**:

```python
def predict(model, data, device):
    model.eval()  # è¯„ä¼°æ¨¡å¼
    
    # 1. æ•°æ®ç±»å‹è½¬æ¢
    if not torch.is_tensor(data):
        data = torch.from_numpy(data).float()
    
    # 2. å¢åŠ æ‰¹æ¬¡ç»´åº¦(å¦‚éœ€è¦)
    if data.dim() == 2:  # (window_size, features)
        data = data.unsqueeze(0)  # â†’ (1, window_size, features)
    
    # 3. æ— æ¢¯åº¦é¢„æµ‹
    with torch.no_grad():
        predictions = model(data)  # (N, predict_steps, 1)
    
    # 4. æ ¼å¼è½¬æ¢ç”¨äºåå½’ä¸€åŒ–
    # å°†GDPé¢„æµ‹å€¼å¡«å……åˆ°4ç»´æ•°ç»„çš„ç¬¬3åˆ—
    pre_2d = predictions.squeeze(-1)[:, 0].reshape(-1, 1)
    temp_inverse_input = np.zeros((pre_2d.shape[0], 4))
    temp_inverse_input[:, 2] = pre_2d.flatten()  # GDPåœ¨ç¬¬3åˆ—
    
    return temp_inverse_input
```

**è¿”å›æ ¼å¼**:

```python
# å½¢çŠ¶: (N_sequences, 4)
# ç¤ºä¾‹è¾“å‡º:
array([[0.   , 0.   , é¢„æµ‹å€¼1, 0.   ],
       [0.   , 0.   , é¢„æµ‹å€¼2, 0.   ]])
```

---

## 4. utils.py - å·¥å…·å‡½æ•°æ¨¡å—

### 4.1 åºåˆ—æ„å»º

#### `create_training_sequences()`

**æ»‘åŠ¨çª—å£åŸç†**:

```
æ—¶é—´åºåˆ—: [2005, 2006, 2007, ..., 2023, 2024]
çª—å£å¤§å°: 6
é¢„æµ‹æ­¥æ•°: 2

ç”Ÿæˆæ ·æœ¬:
X[0] = [2005-2010] â†’ Y[0] = [2011, 2012]
X[1] = [2006-2011] â†’ Y[1] = [2012, 2013]
...
X[N] = [2018-2023] â†’ Y[N] = [2024, 2025]
```

**ä»£ç é€»è¾‘**:

```python
N_total = 20  # æ•°æ®æ€»é•¿åº¦
input_window = 6
output_steps = 2
N_sequences = N_total - input_window - output_steps + 1  # = 13

for i in range(N_sequences):
    x_sequence = data[i : i+6, :]       # (6, 4)
    y_sequence = data[i+6 : i+8, 2]     # (2,) - ä»…GDPåˆ—
    X.append(x_sequence)
    Y.append(y_sequence)

# æœ€ç»ˆå½¢çŠ¶:
# X: (13, 6, 4)
# Y: (13, 2, 1)
```

### 4.2 å¯è§†åŒ–å‡½æ•°

#### `plot_training_comparison()`

**åŠŸèƒ½**: å¯¹æ¯”è®­ç»ƒé›†çœŸå®å€¼ä¸é¢„æµ‹å€¼

**ç»˜å›¾æµç¨‹**:

```python
1. æå–GDPåˆ—(ç´¢å¼•2)
   - true_gdp: data_y_true[:, 2]
   - pred_gdp: data_y_pred[:, 2]

2. å¯¹é½å¹´ä»½
   - èµ·å§‹å¹´ä»½ = å†å²æ•°æ®å¹´ä»½ + window_size
   - ç¤ºä¾‹: 2005æ•°æ®,çª—å£6 â†’ ä»2011å¹´å¼€å§‹å¯¹æ¯”

3. ç»˜åˆ¶åŒçº¿å›¾
   - è“è‰²å®çº¿: çœŸå®å€¼
   - çº¢è‰²è™šçº¿: é¢„æµ‹å€¼
```

#### `plot_gdp_comparison()`

**åŠŸèƒ½**: ç»˜åˆ¶å†å²æ•°æ®+æœªæ¥é¢„æµ‹è¶‹åŠ¿å›¾

**ç‰¹è‰²æ ‡æ³¨**:

```python
- ç°è‰²é€æ˜çº¿: å…¨éƒ¨æ•°æ®æ¦‚è§ˆ
- è“è‰²å®çº¿: å†å²çœŸå®å€¼
- çº¢è‰²è™šçº¿: æœªæ¥é¢„æµ‹å€¼
- é»‘è‰²æ•£ç‚¹: å†å²ä¸é¢„æµ‹åˆ†ç•Œç‚¹
```

---

## 5. train.py - ä¸»è®­ç»ƒè„šæœ¬

### 5.1 è¶…å‚æ•°é…ç½®

```python
# æ¨¡å‹ç»“æ„å‚æ•°
INPUT_FEATURE_SIZE = 4      # è¾“å…¥ç‰¹å¾(äººå£/æ¶ˆè´¹/GDP/è´¢æ”¿)
OUTPUT_FEATURE_SIZE = 1     # è¾“å‡ºç‰¹å¾(ä»…GDP)
HIDDEN_SIZE = 8             # LSTMéšè—å±‚
NUM_LAYERS = 2              # LSTMå †å å±‚æ•°
PREDICT_STEPS = 2           # é¢„æµ‹æœªæ¥2å¹´

# è®­ç»ƒå‚æ•°
WINDOW_SIZE = 6             # å†å²çª—å£6å¹´
BATCH_SIZE = 1              # æ‰¹æ¬¡å¤§å°
NUM_EPOCHS = 100            # è®­ç»ƒè½®æ•°
LEARNING_RATE = 0.001       # Adamå­¦ä¹ ç‡

# æ•°æ®é…ç½®
GDP_COL_INDEX = 2           # GDPåœ¨æ•°æ®ä¸­çš„åˆ—ç´¢å¼•
PROVINCES = ["åŒ—äº¬å¸‚", ...]  # 31ä¸ªçœä»½åˆ—è¡¨
```

### 5.2 è®­ç»ƒæµç¨‹å›¾

```mermaid
graph TD
    A[å¼€å§‹] --> B[éå†31ä¸ªçœä»½]
    B --> C[åŠ è½½æ•°æ®å¹¶å½’ä¸€åŒ–]
    C --> D[æ„å»ºè®­ç»ƒåºåˆ—]
    D --> E[åˆå§‹åŒ–Seq2Seqæ¨¡å‹]
    E --> F[å®šä¹‰æŸå¤±å‡½æ•°MSE]
    F --> G[è®­ç»ƒ100è½®]
    G --> H[ä¿å­˜è®­ç»ƒæŒ‡æ ‡JSON]
    H --> I[è®­ç»ƒé›†é¢„æµ‹å¯è§†åŒ–]
    I --> J[æœªæ¥2å¹´é¢„æµ‹]
    J --> K[ä¿å­˜æ¨¡å‹pth/ONNX]
    K --> L[ä¸‹ä¸€ä¸ªçœä»½]
    L --> |æœªå®Œæˆ| B
    L --> |å…¨éƒ¨å®Œæˆ| M[ç»“æŸ]
```

### 5.3 æ ¸å¿ƒä»£ç è§£æ

#### æ•°æ®å‡†å¤‡é˜¶æ®µ

```python
# 1. åŠ è½½å¹¶å½’ä¸€åŒ–æ•°æ®
origin_data, data = data_setup.create_dataset(data_dir, PROVINCE)
scaler = MinMaxScaler()
data_scaled = scaler.fit_transform(origin_data.values)

# 2. æ„å»ºè®­ç»ƒåºåˆ—
data_x, data_y, val_Y2025, val_Y2026 = utils.create_training_sequences(
    data, WINDOW_SIZE, PREDICT_STEPS, GDP_COL_INDEX
)
# data_x: (13, 6, 4) - 13ä¸ªè®­ç»ƒæ ·æœ¬
# data_y: (13, 2, 1) - å¯¹åº”çš„æ ‡ç­¾
```

#### è®­ç»ƒä¸ä¿å­˜é˜¶æ®µ

```python
# 3. è®­ç»ƒæ¨¡å‹
results = engine.train(model, data_x, data_y, loss_fn, optimizer, NUM_EPOCHS, device)

# 4. ä¿å­˜è®­ç»ƒæŒ‡æ ‡
metrics_payload = {
    'province': PROVINCE,
    'num_epochs': NUM_EPOCHS,
    'metrics': results  # åŒ…å«100è½®çš„loss/mae/mse/mape
}
with open(metrics_path, 'w', encoding='utf-8') as f:
    json.dump(metrics_payload, f, ensure_ascii=False, indent=2)
```

#### é¢„æµ‹ä¸å¯è§†åŒ–é˜¶æ®µ

```python
# 5. è®­ç»ƒé›†é¢„æµ‹å¯¹æ¯”
train_predictions_scaled = engine.predict(model, data_x, device)
train_predictions_unscaled = scaler.inverse_transform(train_predictions_scaled)

utils.plot_training_comparison(
    origin_data, data_x, train_predictions_unscaled, 
    data_y_true_unscaled, PROVINCE, WINDOW_SIZE, 'GDP'
)

# 6. æœªæ¥é¢„æµ‹
final = engine.predict(model, val_Y2025, device)
final_data = scaler.inverse_transform(final)
final_gdp_values = final_data[:, GDP_COL_INDEX]

# è®¡ç®—é¢„æµ‹å¹´ä»½
start_year = origin_data.index[-1] + 1  # 2024 â†’ 2025
years = range(start_year, start_year + len(final_gdp_values))  # [2025, 2026]
```

#### æ¨¡å‹å¯¼å‡ºé˜¶æ®µ

```python
# 7. ä¿å­˜PyTorchæ¨¡å‹
torch.save(model.state_dict(), f"{PROVINCE}_seq2seq_gdp_model.pth")

# 8. å¯¼å‡ºONNXæ ¼å¼
dummy_input = torch.randn(BATCH_SIZE, WINDOW_SIZE, INPUT_FEATURE_SIZE).to(device)
torch.onnx.export(
    model, dummy_input, onnx_model_path,
    opset_version=17,
    input_names=['input'],
    output_names=['output'],
    dynamic_axes={'input': {0: 'batch_size'}, 'output': {0: 'batch_size'}}
)
```

---

## ä½¿ç”¨æŒ‡å—

### å¿«é€Ÿå¼€å§‹

#### 1. ç¯å¢ƒå‡†å¤‡

```bash
# å®‰è£…ä¾èµ–
pip install torch pandas numpy scikit-learn matplotlib onnx tqdm

# å‡†å¤‡æ•°æ®æ–‡ä»¶
data/
â”œâ”€â”€ YearPeople.csv
â”œâ”€â”€ YearXiaofei.csv
â”œâ”€â”€ YearGDP.csv
â””â”€â”€ YearFinancial.csv
```

#### 2. æ•°æ®æ ¼å¼è¦æ±‚

CSVæ–‡ä»¶æ ¼å¼(ç¤ºä¾‹):

| å¹´ä»½ | åŒ—äº¬å¸‚ | å¤©æ´¥å¸‚ | ... |
|------|--------|--------|-----|
| 2005 | 1633.0 | 1043.0 | ... |
| 2006 | 1695.0 | 1075.0 | ... |

#### 3. è¿è¡Œè®­ç»ƒ

```bash
python train.py
```

#### 4. è¾“å‡ºç»“æœ

```
models/
â”œâ”€â”€ åŒ—äº¬å¸‚_seq2seq_gdp_model.pth         # PyTorchæ¨¡å‹
â”œâ”€â”€ åŒ—äº¬å¸‚_seq2seq_gdp_model.onnx        # ONNXæ¨¡å‹
â”œâ”€â”€ åŒ—äº¬å¸‚_training_metrics.json        # è®­ç»ƒæŒ‡æ ‡
â””â”€â”€ ...
```

### è‡ªå®šä¹‰é…ç½®

#### ä¿®æ”¹é¢„æµ‹å¹´æ•°

```python
# train.pyä¸­ä¿®æ”¹
PREDICT_STEPS = 3  # æ”¹ä¸ºé¢„æµ‹æœªæ¥3å¹´
```

#### è°ƒæ•´å†å²çª—å£

```python
WINDOW_SIZE = 10  # ä½¿ç”¨è¿‡å»10å¹´æ•°æ®é¢„æµ‹
```

#### æ›´æ”¹æ¨¡å‹å®¹é‡

```python
HIDDEN_SIZE = 16   # å¢åŠ éšè—å±‚ç¥ç»å…ƒ
NUM_LAYERS = 3     # å¢åŠ LSTMå±‚æ•°
```

---

## APIå‚è€ƒ

### data_setupæ¨¡å—

```python
create_dataset(data_path: str, province: str) -> Tuple[DataFrame, ndarray]
"""
å‚æ•°:
  data_path: æ•°æ®æ–‡ä»¶å¤¹è·¯å¾„
  province: çœä»½åç§°
  
è¿”å›:
  origin_data_reversed: åŸå§‹æ•°æ®(æ—¶é—´é€†åº)
  data_scaled: å½’ä¸€åŒ–æ•°æ®[0,1]
"""
```

### engineæ¨¡å—

```python
train(
    model: nn.Module,
    data_x: ndarray,
    data_y: ndarray,
    loss_fn: nn.Module,
    optimizer: torch.optim.Optimizer,
    num_epochs: int,
    device: str
) -> Dict[str, List[float]]
"""
è¿”å›å­—å…¸åŒ…å«é”®:
  - train_loss, train_mae, train_mse, train_mape
  - test_loss, test_mae, test_mse, test_mape
"""

predict(model: nn.Module, data: ndarray, device: str) -> ndarray
"""
è¿”å›å½¢çŠ¶: (N_sequences, 4)
ç¬¬3åˆ—(ç´¢å¼•2)ä¸ºGDPé¢„æµ‹å€¼
"""
```

### utilsæ¨¡å—

```python
create_training_sequences(
    data: ndarray,
    input_window: int,
    output_steps: int,
    target_index: int
) -> Tuple[ndarray, ndarray, ndarray, ndarray]
"""
è¿”å›:
  X_train: (N, input_window, 4)
  Y_train: (N, output_steps, 1)
  X_val_Y2025: (input_window, 4)
  X_val_Y2026: (input_window, 4)
"""
```

---

## å¸¸è§é—®é¢˜

### Q1: ä¸ºä»€ä¹ˆè¦æ—¶é—´é€†åº?

**A**: ä½¿æœ€æ–°æ•°æ®ç´¢å¼•æœ€å¤§,ä¾¿äºæ„å»ºéªŒè¯é›†(å¦‚2024å¹´æ•°æ®ç”¨äºé¢„æµ‹2025-2026)ã€‚

### Q2: ä¸ºä»€ä¹ˆpredictè¿”å›4åˆ—?

**A**: ä¸ºäº†ä¸scalerçš„inverse_transformå…¼å®¹,éœ€è¦4ç»´è¾“å…¥ã€‚å®é™…åªæœ‰GDPåˆ—æœ‰å€¼ã€‚

### Q3: è®­ç»ƒé›†å’Œæµ‹è¯•é›†ç›¸åŒ?

**A**: æ˜¯çš„,å½“å‰ä»£ç ä¸­train_stepå’Œtest_stepä½¿ç”¨ç›¸åŒæ•°æ®,ä¸»è¦ç”¨äºç›‘æ§è¿‡æ‹Ÿåˆã€‚å®é™…åº”ç”¨åº”åˆ†ç¦»éªŒè¯é›†ã€‚

### Q4: å¦‚ä½•è¯„ä¼°æ¨¡å‹å¥½å?

**A**: ä¸»è¦çœ‹MAPEæŒ‡æ ‡,é€šå¸¸<10%ä¸ºè‰¯å¥½,ç»“åˆå¯è§†åŒ–å¯¹æ¯”å›¾ç›´è§‚åˆ¤æ–­ã€‚

---

## æ€§èƒ½ä¼˜åŒ–å»ºè®®

### 1. æ•°æ®å¢å¼º

```python
# æ·»åŠ æ›´å¤šç‰¹å¾
- å¤±ä¸šç‡
- CPIæŒ‡æ•°
- è¿›å‡ºå£é¢
```

### 2. æ¨¡å‹æ”¹è¿›

```python
# ä½¿ç”¨æ³¨æ„åŠ›æœºåˆ¶
class Seq2SeqWithAttention(nn.Module):
    def __init__(self, ...):
        self.attention = nn.MultiheadAttention(...)
```

### 3. è®­ç»ƒç­–ç•¥

```python
# å­¦ä¹ ç‡è°ƒåº¦
scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
    optimizer, mode='min', factor=0.5, patience=10
)
```

---

## ç‰ˆæœ¬æ›´æ–°æ—¥å¿—

### v1.0.0 (2025-12-20)
- âœ… å®ç°åŸºç¡€Seq2Seqæ¨¡å‹
- âœ… æ”¯æŒ31ä¸ªçœä»½GDPé¢„æµ‹
- âœ… æ·»åŠ ONNXå¯¼å‡ºåŠŸèƒ½
- âœ… å®ç°è®­ç»ƒæŒ‡æ ‡å¯è§†åŒ–

---

## è®¸å¯è¯

MIT License

---

**æ–‡æ¡£ç¼–å†™**: zzzauly
**æœ€åæ›´æ–°**: 2025-12-20  
**ç»´æŠ¤è€…**: é¡¹ç›®å›¢é˜Ÿ